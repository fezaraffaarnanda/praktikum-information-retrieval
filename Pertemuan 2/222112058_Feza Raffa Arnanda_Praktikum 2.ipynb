{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Struktur Data List dan Dictionary pada Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kamu\n"
     ]
    }
   ],
   "source": [
    "list1 = [\"Wilayah\", \"Kamu\",\"Sudah\",\"Bebas\",\"Covid-19\",\"?\"]\n",
    "print(list1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 'Tiga', 'Empat', 5]\n"
     ]
    }
   ],
   "source": [
    "list2 = [1,2,\"Tiga\",\"Empat\",5]\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilayah\n",
      "Kamu\n",
      "Sudah\n",
      "Bebas\n",
      "Covid-19\n",
      "?\n",
      "\n",
      "Atau\n",
      "\n",
      "1\n",
      "2\n",
      "Tiga\n",
      "Empat\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Menampilkan semua anggota list\n",
    "for i in range(len(list1)):\n",
    "    print(list1[i])\n",
    "\n",
    "print(\"\\nAtau\\n\")\n",
    "\n",
    "\n",
    "for list in list2:\n",
    "    print(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilayah\n",
      "Kamu\n",
      "Sudah\n",
      "Bebas\n",
      "COVID-19\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "dict = {1:\"Wilayah\",2:\"Kamu\",3:\"Sudah\",4:\"Bebas\",5:\"COVID-19\",6:\"?\"}\n",
    "\n",
    "for key in dict:\n",
    "    print(dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Implementasi Struktur Data Python untuk Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Pengembangan sistem informasi penjadwalan\"\n",
    "doc1_term = [\"pengembangan\",\"sistem\",\"informasi\",\"penjadwalan\"]\n",
    "doc2 = \"pengembangan model analisis sentimen berita\"\n",
    "doc2_term = [\"pengembangan\",\"model\",\"analisi\",\"sentimen\",\"berita\"]\n",
    "doc3 = \"pengembangan analisis sistem input output\"\n",
    "doc3_term = [\"pengembangan\",\"analisis\",\"sistem\",\"input\",\"output\"]\n",
    "\n",
    "corpus = [doc1,doc2,doc3]\n",
    "corpus_term = [doc1_term,doc2_term,doc3_term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pengembangan': 3, 'sistem': 2, 'informasi': 1, 'penjadwalan': 1, 'model': 1, 'analisi': 1, 'sentimen': 1, 'berita': 1, 'analisis': 1, 'input': 1, 'output': 1}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {}\n",
    "\n",
    "for d in corpus_term:\n",
    "    for term in d:\n",
    "        if term not in vocabulary:\n",
    "            vocabulary[term] = 1\n",
    "        else:\n",
    "            vocabulary[term] = vocabulary[term] + 1\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Tokenisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenisasi(text):\n",
    "    tokens=text.split(\" \")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pengembangan', 'sistem', 'informasi', 'penjadwalan']\n",
      "['pengembangan', 'model', 'analisis', 'sentimen', 'berita']\n",
      "['pengembangan', 'analisis', 'sistem', 'input', 'output']\n"
     ]
    }
   ],
   "source": [
    "for d in corpus:\n",
    "    token_kata = tokenisasi(d)\n",
    "    print(token_kata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pengembangan', 'sistem', 'informasi', 'penjadwalan']\n",
      "['pengembangan', 'model', 'analisis', 'sentimen', 'berita']\n",
      "['pengembangan', 'analisis', 'sistem', 'input', 'output']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.id import Indonesian\n",
    "\n",
    "nlp = Indonesian()\n",
    "# nlp = spacy.blank('id')\n",
    "\n",
    "for d in corpus:\n",
    "    spacy_id = nlp(d)\n",
    "    token_kata = [token.text for token in spacy_id]\n",
    "    print(token_kata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Capitalization/Case-Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WILAYAH KAMU SUDAH 'BEBAS' COVID-19? CEK 34 KAB/KOTA ZONA HIJAU TERBARU\n",
      "wilayah kamu sudah 'bebas' covid-19? cek 34 kab/kota zona hijau terbaru\n"
     ]
    }
   ],
   "source": [
    "text = \"Wilayah Kamu Sudah 'Bebas' COVID-19? Cek 34 Kab/Kota Zona Hijau Terbaru\"\n",
    "\n",
    "text_capital = text.upper()\n",
    "print(text_capital)\n",
    "\n",
    "text_lower = text.lower()\n",
    "print(text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wilayah', 'Kamu', 'sudah', 'Bebas', 'COVID-19', '?', 'Cek', '34', 'Kab', '/', 'Kota', 'Zona', 'Hijau', 'Terbaru']\n"
     ]
    }
   ],
   "source": [
    "stopwords = [\"Yang\", \"Dari\",\"Sudah\",\"Dan\"]\n",
    "text = \"Wilayah Kamu Sudah 'Bebas' COVID-19? Cek 34 Kab/Kota Zona Hijau Terbaru\"\n",
    "\n",
    "tokens = [\"Wilayah\",\"Kamu\",\"sudah\",\"Bebas\",\"COVID-19\",\"?\",\"Cek\",\"34\",\"Kab\",\"/\",\"Kota\",\"Zona\",\"Hijau\",\"Terbaru\"]\n",
    "\n",
    "tokens_nostopword = [w for w in tokens if not w in stopwords]\n",
    "print(tokens_nostopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Normalisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['saya', 'sedang', 'di', 'jalan', 'nih']\n"
     ]
    }
   ],
   "source": [
    "normal_list = {\"gue\": \"saya\",\"gua\":\"saya\",\"aku\":\"saya\",\"aq\":\"saya\",\"lagi\":\"sedang\"}\n",
    "text = \"aq lagi di jalan nih\"\n",
    "text_normal = []\n",
    "\n",
    "for t in text.split(\" \"):\n",
    "    text_normal.append(normal_list[t] if t in normal_list else t)\n",
    "print(text_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wilayah kamu sudah bebas covid-19 cek 34 kab kota zona hijau baru\n"
     ]
    }
   ],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Stemming process\n",
    "text = \"Wilayah Kamu Sudah 'Bebas' COVID-19? Cek 34 Kab/Kota Zona Hijau Terbaru\"\n",
    "output = stemmer.stem(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penugasan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Diketahui suatu dokumen berikut terdiri dari beberapa paragraf dan setiap paragraf\n",
    "terdiri dari beberapa kalimat. Paragraf yang berbeda dipisahkan dengan Enter,\n",
    "sedangkan kalimat dipisahkan dengan titik, tanda tanya, atau tanda seru. Buat kode\n",
    "fungsi python untuk memisahkan dokumen sehingga menghasilkan variabel\n",
    "list_paragraf (nama fungsi: paragraph_parsing), dan masing-masing paragraf\n",
    "menjadi variabel list_kalimat (nama fungsi: sentence_parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_parser(text):\n",
    "    paragraf = text.split('\\n')\n",
    "\n",
    "    parsed_paragraf = []\n",
    "\n",
    "    for indeks, paragraf in enumerate(paragraf):\n",
    "        label = f\"p{indeks+1}\"\n",
    "        parsed_paragraf.append(f\"{label}:{paragraf}\") # untuk pemberian label \"p1\" \"p2\" dll. f itu f-string di python untuk include expression di dalam string.\n",
    "\n",
    "    return \"\\n\\n\".join(parsed_paragraf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sentence_parser(paragraph):\n",
    "    sentences = re.split(r'[.!?]', paragraph)  #re.split untuk split dari suatu teks berdasarkan expression.\n",
    "    \n",
    "    parsed_output = []\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if sentence.strip():  \n",
    "            label = f\"s{index + 1}\"  \n",
    "            parsed_output.append(f\"{label} : {sentence.strip()}\")\n",
    "            \n",
    "    return parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List paragraf : \n",
      "\n",
      "p1:Mobilitas warga bakal diperketat melalui penerapan PPKM level 3 se-Indonesia di masa libur Natal dan tahun baru (Nataru). Rencana kebijakan itu dikritik oleh Epidemiolog dari Griffith University Dicky Budiman.\n",
      "\n",
      "p2:Dicky menyebut pembatasan mobilitas memang akan memiliki dampak dalam mencegah penularan COVID-19. Tapi, kata dia, dampaknya signifikan atau tidak akan bergantung pada konsistensi yang mendasar yakni testing, tracing, treatment, (3T) hingga vaksinasi COVID-19. \n",
      "\n",
      "List kalimat pada paragraf 1 :\n",
      "\n",
      "s1 : Mobilitas warga bakal diperketat melalui penerapan PPKM level 3 se-Indonesia di masa libur Natal dan tahun baru (Nataru)\n",
      "s2 : Rencana kebijakan itu dikritik oleh Epidemiolog dari Griffith University Dicky Budiman\n",
      "\n",
      "List kalimat pada paragraf 2 :\n",
      "\n",
      "s1 : Dicky menyebut pembatasan mobilitas memang akan memiliki dampak dalam mencegah penularan COVID-19\n",
      "s2 : Tapi, kata dia, dampaknya signifikan atau tidak akan bergantung pada konsistensi yang mendasar yakni testing, tracing, treatment, (3T) hingga vaksinasi COVID-19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teks = \"\"\"\n",
    "Mobilitas warga bakal diperketat melalui penerapan PPKM level 3 se-Indonesia di masa libur Natal dan tahun baru (Nataru). Rencana kebijakan itu dikritik oleh Epidemiolog dari Griffith University Dicky Budiman.\n",
    "Dicky menyebut pembatasan mobilitas memang akan memiliki dampak dalam mencegah penularan COVID-19. Tapi, kata dia, dampaknya signifikan atau tidak akan bergantung pada konsistensi yang mendasar yakni testing, tracing, treatment, (3T) hingga vaksinasi COVID-19.\n",
    "\"\"\"\n",
    "\n",
    "# Paragraf parser\n",
    "paragraphs = paragraph_parser(teks.strip())\n",
    "print(\"List paragraf : \\n\")\n",
    "print(paragraphs, \"\\n\")\n",
    "\n",
    "\n",
    "# Kalimat parser\n",
    "paragraphs = teks.strip().split('\\n')  # paragraf parser\n",
    "\n",
    "for index, paragraph in enumerate(paragraphs):\n",
    "    sentence_list = sentence_parser(paragraph)\n",
    "    print(f\"List kalimat pada paragraf {index+ 1} :\\n\")\n",
    "    print('\\n'.join(sentence_list))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Lakukan case-folding (upper case dan lower case), tokenisasi, eliminasi stopword dan stemming pada dokumen di folder “berita” menggunakan library yang sudah tersedia (nltk, spacy, sastrawi, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['terinfeksi', 'viru', 'corona', 'melonjak', 'negara', 'pemerintah', 'kera', 'mengatasi', 'penyebaran', 'viru']\n",
      "['mencuci', 'tangan', 'rutin', 'mencegah', 'penularan', 'penyakit', 'penelitian', 'mencuci', 'tangan', 'mengurangi', 'risiko', 'infeksi']\n",
      "['pandemi', 'corona', 'mengubah', 'aspek', 'kehidupan', 'mencari', 'solusi', 'mengatasi', 'negatifnya']\n",
      "['hasil', 'survei', 'tingkat', 'kepuasan', 'masyarakat', 'layanan', 'kesehatan', 'menurun', 'pandemi', 'perbaikan', 'diambil']\n",
      "['pemerintah', 'mengumumkan', 'kebijakan', 'terkait', 'pembatasan', 'sosial', 'mengendalikan', 'penyebaran', 'viru', 'warga', 'diharapkan', 'mematuhi', 'aturan']\n",
      "['file']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "# Path folder berita\n",
    "path = \"C:/Users/FEZA/My Drive/00. Drive PC/1.STIS/5. Semester 5/Information Retrieval [IR] P/Pertemuan 1/berita\"\n",
    "\n",
    "# Stopwords pada library NLTK\n",
    "\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Stemmer pada library NLTK\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Iterate through the files\n",
    "for file in os.listdir(path):\n",
    "    if os.path.isfile(os.path.join(path, file)):\n",
    "        with open(os.path.join(path, file), 'r', encoding='utf-8') as f:\n",
    "            content = f.read().lower()  # Casefolding pada NLTK\n",
    "            \n",
    "            # Tokenisasi\n",
    "            words = word_tokenize(content)\n",
    "            \n",
    "            # menghilangkan stopwords sekaligus stemming\n",
    "            # memeriksa apakah kata saat ini adalah alfanumerik (terdiri dari huruf dan/atau angka) dan apakah\n",
    "            # kata tersebut bukan termasuk dalam daftar stopwords.\n",
    "            filtered_words = [stemmer.stem(word) for word in words if word.isalnum() and word not in stop_words] \n",
    "        \n",
    "            \n",
    "            # print hasil akhir \n",
    "            print(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy tidak mempunyai model untuk bahasa indonesia, jadi pada penugasan kali ini saya menggunakan spacy model untuk bahasa inggris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iconresource', 'file']\n",
      "['powerful', 'earthquake', 'strike', 'indonesia', 'thursday', 'kill', 'people', 'injure', 'hundred', 'earthquake', 'magnitude', 'strike', 'island', 'sumatra', 'local', 'time', 'epicenter', 'locate', 'mile', 'city', 'padang']\n",
      "['nasa', 'thursday', 'unveil', 'new', 'space', 'telescope', 'james', 'webb', 'space', 'telescope', 'telescope', 'powerful', 'build', 'design', 'study', 'universe', 'unprecedented', 'detail']\n",
      "['new', 'study', 'publish', 'thursday', 'find', 'climate', 'change', 'worsen', 'study', 'conduct', 'team', 'scientist', 'university', 'oxford', 'find', 'earth', 'climate', 'warm', 'alarming', 'rate']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model untuk bahasa inggris. \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Path to the \"berita\" folder\n",
    "path = \"C:/Users/FEZA/My Drive/00. Drive PC/1.STIS/5. Semester 5/Information Retrieval [IR] P/Pertemuan 1/berita/english\"\n",
    "\n",
    "# Iterate through the files\n",
    "for file in os.listdir(path):\n",
    "    if os.path.isfile(os.path.join(path, file)):\n",
    "        with open(os.path.join(path, file), 'r', encoding='utf-8') as f:\n",
    "            content = f.read().lower()  # Convert to lowercase\n",
    "            \n",
    "            # Process text using spaCy\n",
    "            doc = nlp(content)\n",
    "            \n",
    "            # Lemmatize and remove stopwords\n",
    "            processed_words = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "            \n",
    "            # Print the processed words\n",
    "            print(processed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SASTRAWI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kasus', 'baru', 'infeksi', 'virus', 'corona', 'lonjak', '', 'beberapa', 'negara', '', 'perintah', 'sedang', 'kerja', 'keras', '', 'atas', 'sebar', 'virus', '', '']\n",
      "['penting', 'cuci', 'tangan', '', 'rutin', '', 'cegah', 'tular', 'sakit', '', 'teliti', 'tunjuk', '', 'cuci', 'tangan', '', 'kurang', 'risiko', 'infeksi', '']\n",
      "['pandemi', 'corona', '', 'ubah', 'banyak', 'aspek', 'hidup', '', '', '', 'perlu', 'sama', 'cari', 'solusi', '', 'atas', 'dampak', 'negatif']\n",
      "['hasil', 'survei', 'tunjuk', '', 'tingkat', 'puas', 'masyarakat', '', 'layan', 'sehat', '', 'turun', 'lama', 'pandemi', '', 'langkah', 'baik', 'perlu', 'segera', 'ambil']\n",
      "['perintah', 'umum', 'bijak', 'baru', 'kait', 'batas', 'sosial', '', 'kendali', 'sebar', 'virus', '', 'semua', 'warga', 'harap', 'patuh', 'atur', 'sebut', '']\n",
      "['', 'shellclassinfo', '', 'iconresource c', '', 'program', 'files google drive', 'file', 'stream 79 0 2 0 googledrivefs exe 23']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# path ke folder berita\n",
    "path = \"C:/Users/FEZA/My Drive/00. Drive PC/1.STIS/5. Semester 5/Information Retrieval [IR] P/Pertemuan 1/berita\"\n",
    "\n",
    "# methode sastrawi stemmer dan stopword remover\n",
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stopword_remover = stopword_factory.create_stop_word_remover()\n",
    "\n",
    "# Iterai ke semua file di folder berita\n",
    "for file in os.listdir(path):\n",
    "    if os.path.isfile(os.path.join(path, file)):\n",
    "        with open(os.path.join(path, file), 'r', encoding='utf-8') as f:\n",
    "            content = f.read().lower()  # case folding\n",
    "            \n",
    "            # Tokenization\n",
    "            words = word_tokenize(content)\n",
    "            \n",
    "            # mennghilangkan stopword dan dilakukan stemming sekaligus\n",
    "            filtered_words = [stemmer.stem(stopword_remover.remove(word)) for word in words]\n",
    "            \n",
    "            # Print hasil akhir\n",
    "            print(filtered_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
