{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penugasan Praktikum Pertemuan 2 Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feza Raffa Arnanda - 3SD2 - 222112058"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Diketahui suatu dokumen berikut terdiri dari beberapa paragraf dan setiap paragraf terdiri dari beberapa kalimat. Paragraf yang berbeda dipisahkan dengan Enter, sedangkan kalimat dipisahkan dengan titik, tanda tanya, atau tanda seru. Buat kode fungsi python untuk memisahkan dokumen sehingga menghasilkan variabel list_paragraf (nama fungsi: paragraph_parsing), dan masing-masing paragraf menjadi variabel list_kalimat (nama fungsi: sentence_parsing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paragraph parsing function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_parser(text):\n",
    "    paragraf = text.split('\\n')\n",
    "\n",
    "    parsed_paragraf = []\n",
    "\n",
    "    for indeks, paragraf in enumerate(paragraf):\n",
    "        label = f\"p{indeks+1}\"\n",
    "        parsed_paragraf.append(f\"{label}:{paragraf}\") # untuk pemberian label \"p1\" \"p2\" dll. f itu f-string di python untuk include expression di dalam string.\n",
    "\n",
    "    return \"\\n\\n\".join(parsed_paragraf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence parsing function. Menggunakan library re (regular expression) yang akan memudahkan dalam pemberian pola "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sentence_parser(paragraph):\n",
    "    sentences = re.split(r'[.!?]', paragraph)  #re.split untuk split dari suatu teks berdasarkan expression.\n",
    "    \n",
    "    parsed_output = []\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if sentence.strip():  \n",
    "            label = f\"s{index + 1}\"  \n",
    "            parsed_output.append(f\"{label} : {sentence.strip()}\")\n",
    "            \n",
    "    return parsed_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List paragraf : \n",
      "\n",
      "p1:Mobilitas warga bakal diperketat melalui penerapan PPKM level 3 se-Indonesia di masa libur Natal dan tahun baru (Nataru). Rencana kebijakan itu dikritik oleh Epidemiolog dari Griffith University Dicky Budiman.\n",
      "\n",
      "p2:Dicky menyebut pembatasan mobilitas memang akan memiliki dampak dalam mencegah penularan COVID-19. Tapi, kata dia, dampaknya signifikan atau tidak akan bergantung pada konsistensi yang mendasar yakni testing, tracing, treatment, (3T) hingga vaksinasi COVID-19. \n",
      "\n",
      "List kalimat pada paragraf 1 :\n",
      "\n",
      "s1 : Mobilitas warga bakal diperketat melalui penerapan PPKM level 3 se-Indonesia di masa libur Natal dan tahun baru (Nataru)\n",
      "s2 : Rencana kebijakan itu dikritik oleh Epidemiolog dari Griffith University Dicky Budiman\n",
      "\n",
      "List kalimat pada paragraf 2 :\n",
      "\n",
      "s1 : Dicky menyebut pembatasan mobilitas memang akan memiliki dampak dalam mencegah penularan COVID-19\n",
      "s2 : Tapi, kata dia, dampaknya signifikan atau tidak akan bergantung pada konsistensi yang mendasar yakni testing, tracing, treatment, (3T) hingga vaksinasi COVID-19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teks = \"\"\"\n",
    "Mobilitas warga bakal diperketat melalui penerapan PPKM level 3 se-Indonesia di masa libur Natal dan tahun baru (Nataru). Rencana kebijakan itu dikritik oleh Epidemiolog dari Griffith University Dicky Budiman.\n",
    "Dicky menyebut pembatasan mobilitas memang akan memiliki dampak dalam mencegah penularan COVID-19. Tapi, kata dia, dampaknya signifikan atau tidak akan bergantung pada konsistensi yang mendasar yakni testing, tracing, treatment, (3T) hingga vaksinasi COVID-19.\n",
    "\"\"\"\n",
    "\n",
    "# Paragraf parser\n",
    "paragraphs = paragraph_parser(teks.strip())\n",
    "print(\"List paragraf : \\n\")\n",
    "print(paragraphs, \"\\n\")\n",
    "\n",
    "\n",
    "# Kalimat parser\n",
    "paragraphs = teks.strip().split('\\n')  # paragraf parser\n",
    "\n",
    "for index, paragraph in enumerate(paragraphs):\n",
    "    sentence_list = sentence_parser(paragraph)\n",
    "    print(f\"List kalimat pada paragraf {index+ 1} :\\n\")\n",
    "    print('\\n'.join(sentence_list))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Lakukan case-folding (upper case dan lower case), tokenisasi, eliminasi stopword dan stemming pada dokumen di folder “berita” menggunakan library yang sudah tersedia (nltk, spacy, sastrawi, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada library NLTK, tersedia versi bahasa indonesia untuk melakukan text processing (case folding, tokenisasi, stemming, dll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['terinfeksi', 'viru', 'corona', 'melonjak', 'negara', 'pemerintah', 'kera', 'mengatasi', 'penyebaran', 'viru']\n",
      "['mencuci', 'tangan', 'rutin', 'mencegah', 'penularan', 'penyakit', 'penelitian', 'mencuci', 'tangan', 'mengurangi', 'risiko', 'infeksi']\n",
      "['pandemi', 'corona', 'mengubah', 'aspek', 'kehidupan', 'mencari', 'solusi', 'mengatasi', 'negatifnya']\n",
      "['hasil', 'survei', 'tingkat', 'kepuasan', 'masyarakat', 'layanan', 'kesehatan', 'menurun', 'pandemi', 'perbaikan', 'diambil']\n",
      "['pemerintah', 'mengumumkan', 'kebijakan', 'terkait', 'pembatasan', 'sosial', 'mengendalikan', 'penyebaran', 'viru', 'warga', 'diharapkan', 'mematuhi', 'aturan']\n",
      "['file']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Path folder berita\n",
    "path = \"C:/Users/FEZA/My Drive/00. Drive PC/1.STIS/5. Semester 5/Information Retrieval [IR] P/Pertemuan 1/berita\"\n",
    "\n",
    "# Stopwords pada library NLTK\n",
    "\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Stemmer pada library NLTK\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Iterasi\n",
    "for file in os.listdir(path):\n",
    "    if os.path.isfile(os.path.join(path, file)):\n",
    "        with open(os.path.join(path, file), 'r', encoding='utf-8') as f:\n",
    "            content = f.read().lower()  # Casefolding pada NLTK\n",
    "            \n",
    "            # Tokenisasi\n",
    "            words = word_tokenize(content)\n",
    "            \n",
    "            # menghilangkan stopwords sekaligus stemming\n",
    "            # memeriksa apakah kata saat ini adalah alfanumerik (terdiri dari huruf dan/atau angka) dan apakah\n",
    "            # kata tersebut bukan termasuk dalam daftar stopwords.\n",
    "            filtered_words = [stemmer.stem(word) for word in words if word.isalnum() and word not in stop_words] \n",
    "        \n",
    "            \n",
    "            # print hasil akhir \n",
    "            print(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sastrawi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada library Sastrawi tersedia juga versi bahasa Indonesia sehingga lebih mudah untuk text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kasus', 'baru', 'infeksi', 'virus', 'corona', 'lonjak', '', 'beberapa', 'negara', '', 'perintah', 'sedang', 'kerja', 'keras', '', 'atas', 'sebar', 'virus', '', '']\n",
      "['penting', 'cuci', 'tangan', '', 'rutin', '', 'cegah', 'tular', 'sakit', '', 'teliti', 'tunjuk', '', 'cuci', 'tangan', '', 'kurang', 'risiko', 'infeksi', '']\n",
      "['pandemi', 'corona', '', 'ubah', 'banyak', 'aspek', 'hidup', '', '', '', 'perlu', 'sama', 'cari', 'solusi', '', 'atas', 'dampak', 'negatif']\n",
      "['hasil', 'survei', 'tunjuk', '', 'tingkat', 'puas', 'masyarakat', '', 'layan', 'sehat', '', 'turun', 'lama', 'pandemi', '', 'langkah', 'baik', 'perlu', 'segera', 'ambil']\n",
      "['perintah', 'umum', 'bijak', 'baru', 'kait', 'batas', 'sosial', '', 'kendali', 'sebar', 'virus', '', 'semua', 'warga', 'harap', 'patuh', 'atur', 'sebut', '']\n",
      "['', 'shellclassinfo', '', 'iconresource c', '', 'program', 'files google drive', 'file', 'stream 79 0 2 0 googledrivefs exe 23']\n"
     ]
    }
   ],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# path ke folder berita\n",
    "path = \"C:/Users/FEZA/My Drive/00. Drive PC/1.STIS/5. Semester 5/Information Retrieval [IR] P/Pertemuan 1/berita\"\n",
    "\n",
    "# methode sastrawi stemmer dan stopword remover\n",
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stopword_remover = stopword_factory.create_stop_word_remover()\n",
    "\n",
    "# Iterai ke semua file di folder berita\n",
    "for file in os.listdir(path):\n",
    "    if os.path.isfile(os.path.join(path, file)):\n",
    "        with open(os.path.join(path, file), 'r', encoding='utf-8') as f:\n",
    "            content = f.read().lower()  # case folding\n",
    "            \n",
    "            # Tokenization\n",
    "            words = word_tokenize(content)\n",
    "            \n",
    "            # mennghilangkan stopword dan dilakukan stemming sekaligus\n",
    "            filtered_words = [stemmer.stem(stopword_remover.remove(word)) for word in words]\n",
    "            \n",
    "            # Print hasil akhir\n",
    "            print(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada library SpaCy tidak tersedia versi bahasa Indonesia, sehingga disini saya menggunakan versi bahasa inggris dengan folder berita yang sudah berisi file berbahasa inggris juga, sehingga penggunaan library Spacy bisa kita gunakan secara maksimal dan terlihat bagaimana proses text processing yang baik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iconresource', 'file']\n",
      "['powerful', 'earthquake', 'strike', 'indonesia', 'thursday', 'kill', 'people', 'injure', 'hundred', 'earthquake', 'magnitude', 'strike', 'island', 'sumatra', 'local', 'time', 'epicenter', 'locate', 'mile', 'city', 'padang']\n",
      "['nasa', 'thursday', 'unveil', 'new', 'space', 'telescope', 'james', 'webb', 'space', 'telescope', 'telescope', 'powerful', 'build', 'design', 'study', 'universe', 'unprecedented', 'detail']\n",
      "['new', 'study', 'publish', 'thursday', 'find', 'climate', 'change', 'worsen', 'study', 'conduct', 'team', 'scientist', 'university', 'oxford', 'find', 'earth', 'climate', 'warm', 'alarming', 'rate']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model untuk bahasa inggris. \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# path ke berita folder berbahasa inggris\n",
    "path = \"C:/Users/FEZA/My Drive/00. Drive PC/1.STIS/5. Semester 5/Information Retrieval [IR] P/Pertemuan 1/berita/english\"\n",
    "\n",
    "# Iterasi \n",
    "for file in os.listdir(path):\n",
    "    if os.path.isfile(os.path.join(path, file)):\n",
    "        with open(os.path.join(path, file), 'r', encoding='utf-8') as f:\n",
    "            content = f.read().lower()  # Casefolding\n",
    "            \n",
    "            # Proses NLP pada library Spacy\n",
    "            doc = nlp(content)\n",
    "            \n",
    "            # Lemmatize (Stemming) dan penghilangan stopwords\n",
    "            processed_words = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "            \n",
    "            # Print output\n",
    "            print(processed_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
