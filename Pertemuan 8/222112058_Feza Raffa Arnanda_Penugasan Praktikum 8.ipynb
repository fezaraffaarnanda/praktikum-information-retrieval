{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penugasan Praktikum Pertemuan 8 Information Retrieval\n",
    "\n",
    "Nama    : Feza Raffa Arnanda\n",
    "\n",
    "NIM     : 222112058\n",
    "\n",
    "Kelas   : 3SD2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soal \n",
    "Buat fungsi main untuk menampilkan 3 list dokumen yang terurut berdasarkan BM25 pada folder ”berita” dengan query ”vaksin corona jakarta”. Bandingkan dengan hasil perankingan cosine similarity pada modul 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import package BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenisasi(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    # create stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    # stemming process\n",
    "    output = stemmer.stem(text)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_sentence(text):\n",
    "    stemmed_tokens = [stemming(token) for token in tokenisasi(text)]\n",
    "    output = ' '.join(stemmed_tokens)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "def stemming_sastrawi(tokens):\n",
    "    # Membuat stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_folding(text):\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('Indonesian'))\n",
    "\n",
    "def eliminasi_stopword(token):\n",
    "    return [kata for kata in token if kata not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path folder berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/FEZA/My Drive/00. Drive PC/1.STIS/5. Semester 5/Information Retrieval [IR] P/berita\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open file pada folder berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import os\n",
    "inverted_index = {}\n",
    "doc_dict = {}\n",
    "i = 1\n",
    "for filename in os.listdir(path):\n",
    "    if (filename.endswith('.txt')):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        # Ekstrak angka dari nama file menggunakan regular expressions\n",
    "        match = re.search(r'\\d+', filename)\n",
    "        if match:\n",
    "            doc_id = match.group() # Mengambil angka dari nama file sebagai dokumen ID\n",
    "            with open (file_path, mode='r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                hasil_case_folding = case_folding(text)\n",
    "                token = tokenisasi(hasil_case_folding)\n",
    "                token_bersih = eliminasi_stopword(token)\n",
    "                stemm_token = stemming_sastrawi(token_bersih)\n",
    "                stemm_token_final = [item for item in stemm_token if item != ''] # membersihkan term kosong pada hasil stemming sebelumnya\n",
    "                # Menggabungkan hasil stemming menjadi sebuah teks/paragraf\n",
    "                doc_dict[doc_id] = ' '.join(stemm_token_final)\n",
    "                for term in set(stemm_token_final): # penggunaan set untuk mengantisipasi duplikasi term pada sebuah dokumen\n",
    "                    if term in inverted_index:\n",
    "                        inverted_index[term].append(doc_id)\n",
    "                    else:\n",
    "                        inverted_index[term] = [doc_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat tokenized corpus, dimana akan dibuat list hasil preprocessing teks pada setiap dokumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [tokenisasi(doc_dict[doc_id]) for doc_id in doc_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inisiaslisasi query dan hasil skor menggunakan BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17910081 0.80460786 1.10319823 0.35216875 0.53675825]\n"
     ]
    }
   ],
   "source": [
    "query = \"vaksin corona jakarta\"\n",
    "\n",
    "tokenized_query = tokenisasi(query)\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "print(doc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function untuk me-retrieve top K dokumen dari hasil skore BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def exact_top_k(doc_dict, rank_score, k):\n",
    "    relevance_scores = {}\n",
    "    i = 0\n",
    "    for doc_id in doc_dict.keys():\n",
    "        relevance_scores[doc_id] = rank_score[i]\n",
    "        i = i + 1\n",
    "    sorted_value = OrderedDict(sorted(relevance_scores.items(), key=lambda x: x[1], reverse = True))\n",
    "    top_k = {j: sorted_value[j] for j in list(sorted_value)[:k]}\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = exact_top_k(doc_dict,doc_scores,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print hasil ranking document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: berita3.txt, Skor: 1.1031982334570316\n",
      "Rank 2: berita2.txt, Skor: 0.8046078586167857\n",
      "Rank 3: berita5.txt, Skor: 0.5367582471433582\n"
     ]
    }
   ],
   "source": [
    "for i, (doc_id, score) in enumerate(top_k.items()):\n",
    "    print(f\"Rank {i + 1}: berita{doc_id}.txt, Skor: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil perankingan top 3 dokumen:\n",
    "1. berita2.txt dengan nilai cosine similarity = [0.19658299]\n",
    "2. berita3.txt dengan nilai cosine similarity = [0.13375627]\n",
    "3. berita5.txt dengan nilai cosine similarity = [0.0569143]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kesimpulan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan hasil antara SVM menggunakan BM25 dan Cosine Similarity, top 3 dokumen yang dihasilkan berbeda pada ranking 1 dan 2 yang terbalik. Pada hasil menggunakan BM25, urutan ranked documentnya adalah berita3, berita2, dan berita5. Sedangkan jika menggunakan cosine similarity, urutan ranked documentnya adalah berita3, berita4, dan berita2. \n",
    "\n",
    "Pengunaan BM25 berdasarkan teori pada materi perkuliahan lebih baik daripada menggunakan bobot TF.IDf, karena pada BM25 memperhitungkan panjang dokumen dari Term Frequency, sedangkan TF.IDf tidak memperhitungkan itu. BM25 lebih baik dalam penanganan term yang jarang muncul pada dokumen sehingga dapat menghasilkan top K document yang lebih relevan dan baik."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
