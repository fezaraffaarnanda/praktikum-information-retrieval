{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referensi : https://github.com/peermohtaram/Vector-Space-Model/blob/master/Vector_Space_Model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termFrequencyInDoc(vocab, doc_dict):\n",
    "    tf_docs = {}\n",
    "    for doc_id in doc_dict.keys():\n",
    "        tf_docs[doc_id] = {}\n",
    "    for word in vocab:\n",
    "        for doc_id,doc in doc_dict.items():\n",
    "            tf_docs[doc_id][word] = doc.count(word)\n",
    "    return tf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inverted Index ---\n",
      "{'kembang': [1, 2], 'sistem': [1, 3], 'informasi': [1], 'jadwal': [1], 'model': [2], 'analisis': [2, 3], 'sentimen': [2], 'berita': [2], 'input': [3], 'output': [3]}\n",
      "\n",
      "\n",
      "--- Term Frequency -- \n",
      "{'doc1': {'kembang': 1, 'sistem': 1, 'informasi': 1, 'jadwal': 1, 'model': 0, 'analisis': 0, 'sentimen': 0, 'berita': 0, 'input': 0, 'output': 0}, 'doc2': {'kembang': 1, 'sistem': 0, 'informasi': 0, 'jadwal': 0, 'model': 1, 'analisis': 1, 'sentimen': 1, 'berita': 1, 'input': 0, 'output': 0}, 'doc3': {'kembang': 0, 'sistem': 1, 'informasi': 0, 'jadwal': 0, 'model': 0, 'analisis': 1, 'sentimen': 0, 'berita': 0, 'input': 1, 'output': 1}}\n"
     ]
    }
   ],
   "source": [
    "doc1_term = [\"pengembangan\", \"sistem\", \"informasi\", \"penjadwalan\"]\n",
    "doc2_term = [\"pengembangan\", \"model\", \"analisis\", \"sentimen\", \"berita\"]\n",
    "doc3_term = [\"analisis\", \"sistem\", \"input\", \"output\"]\n",
    "corpus_term = [doc1_term, doc2_term, doc3_term]\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "\n",
    "inverted_index = {}\n",
    "for i in range(len(corpus_term)):\n",
    "    for item in corpus_term[i]:\n",
    "        item = stemmer.stem(item)\n",
    "        if item not in inverted_index:\n",
    "            inverted_index[item] = []\n",
    "        if (item in inverted_index) and ((i+1) not in inverted_index[item]):\n",
    "            inverted_index[item].append(i+1)\n",
    "\n",
    "print(\"--- Inverted Index ---\")\n",
    "print(inverted_index)\n",
    "print(\"\\n\")\n",
    "\n",
    "vocab = list(inverted_index.keys())\n",
    "doc_dict = {}\n",
    "#clean after stemming\n",
    "doc_dict['doc1'] = \"kembang sistem informasi jadwal\"\n",
    "doc_dict['doc2'] = \"kembang model analisis sentimen berita\"\n",
    "doc_dict['doc3'] = \"analisis sistem input output\"\n",
    "\n",
    "print(\"--- Term Frequency -- \")\n",
    "print(termFrequencyInDoc(vocab, doc_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penjelasan kode di atas : \n",
    "\n",
    "Inverted index untuk list apa saja kata yang mau dicari di suatu document\n",
    "\n",
    "Nah, term frequencynya itu biar kita cari setiap kata yang ada di inverted index di suatu document tuh muncul berapa kalii. Gitu bro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def wordDocFre(vocab, doc_dict):\n",
    "    df = {}\n",
    "    for word in vocab:\n",
    "        frq = 0\n",
    "        for doc in doc_dict.values():\n",
    "          #if word in doc.lower().split():\n",
    "            if word in doc:\n",
    "                frq = frq + 1\n",
    "        df[word] = frq\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency\n",
    "\n",
    "Formula : IDF = log (N/Df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def inverseDocFre(vocab,doc_fre,length):\n",
    "    idf= {}\n",
    "    for word in vocab:\n",
    "        idf[word] = idf[word] = 1 + np.log((length + 1) / (doc_fre[word]+1))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kembang': 1.2876820724517808, 'sistem': 1.2876820724517808, 'informasi': 1.6931471805599454, 'jadwal': 1.6931471805599454, 'model': 1.6931471805599454, 'analisis': 1.2876820724517808, 'sentimen': 1.6931471805599454, 'berita': 1.6931471805599454, 'input': 1.6931471805599454, 'output': 1.6931471805599454}\n"
     ]
    }
   ],
   "source": [
    "print(inverseDocFre(vocab, wordDocFre(vocab, doc_dict),len(doc_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Space Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function dibawah ini menghasilkan w = TF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(vocab,tf,idf_scr,doc_dict):\n",
    "    tf_idf_scr = {}\n",
    "    for doc_id in doc_dict.keys():\n",
    "        tf_idf_scr[doc_id] = {}\n",
    "    for word in vocab:\n",
    "        for doc_id,doc in doc_dict.items():\n",
    "            tf_idf_scr[doc_id][word] = tf[doc_id][word] * idf_scr[word]\n",
    "    return tf_idf_scr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term-Document Matrix\n",
    "<br>\n",
    "    doc1  doc2   doc3\n",
    "<br>\n",
    "t1 |w11   w12    w13|\n",
    "<br>\n",
    "t2 |w21   w22    w23|\n",
    "<br>\n",
    "t3 |w31   w32    w33|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.28768207 1.28768207 0.        ]\n",
      " [1.28768207 0.         1.28768207]\n",
      " [1.69314718 0.         0.        ]\n",
      " [1.69314718 0.         0.        ]\n",
      " [0.         1.69314718 0.        ]\n",
      " [0.         1.28768207 1.28768207]\n",
      " [0.         1.69314718 0.        ]\n",
      " [0.         1.69314718 0.        ]\n",
      " [0.         0.         1.69314718]\n",
      " [0.         0.         1.69314718]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf = tfidf(vocab, termFrequencyInDoc(vocab, doc_dict), inverseDocFre(vocab, wordDocFre(vocab, doc_dict), len(doc_dict)), doc_dict)\n",
    "# Term - Document Matrix\n",
    "TD = np.zeros((len(vocab), len(doc_dict)))\n",
    "for word in vocab:\n",
    "    for doc_id,doc in tf_idf.items():\n",
    "        ind1 = vocab.index(word)\n",
    "        ind2 = list(tf_idf.keys()).index(doc_id)\n",
    "        TD[ind1][ind2] = tf_idf[doc_id][word]\n",
    "print(TD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edit Distace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref : https://www.w3resource.com/python-exercises/challenges/1/python-challenges-1-exercise-52.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(string1, string2):\n",
    "    if len(string1) > len(string2):\n",
    "        difference = len(string1) - len(string2)\n",
    "        string1[:difference]\n",
    "        n = len(string2)\n",
    "    elif len(string2) > len(string1):\n",
    "        difference = len(string2) - len(string1)\n",
    "        string2[:difference]\n",
    "        n = len(string1)\n",
    "    for i in range(n):\n",
    "        if string1[i] != string2[i]:\n",
    "            difference += 1\n",
    "            \n",
    "    return difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "print(edit_distance(doc_dict['doc1'], doc_dict['doc2']))\n",
    "print(edit_distance(doc_dict['doc1'], doc_dict['doc3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref : https://www.w3resource.com/python-exercises/extended-data-types/python-extended-data-types-index-counter-exercise-9.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    \n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n",
      "0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "print(jaccard_sim(doc_dict['doc1'].split(\" \"), doc_dict['doc2'].split(\" \")))\n",
    "print(jaccard_sim(doc_dict['doc1'].split(\" \"), doc_dict['doc3'].split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidian Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_dist(vec1, vec2):\n",
    "    # subtracting vector\n",
    "    temp = vec1 - vec2\n",
    "    # doing dot product\n",
    "    # for finding\n",
    "    # sum of the squares\n",
    "    sum_sq = np.dot(temp.T, temp)\n",
    "    # Doing squareroot and\n",
    "    # printing Euclidean distance\n",
    "    \n",
    "    return np.sqrt(sum_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.201188773980275\n",
      "3.844897884155026\n"
     ]
    }
   ],
   "source": [
    "print(euclidian_dist(TD[:, 0], TD[:, 1])) #doc1 & doc2\n",
    "print(euclidian_dist(TD[:, 0], TD[:, 2])) #doc1 & doc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref : https://algoritmaonline.com/kemiripan-teks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cosine_sim(vec1, vec2):\n",
    "    vec1 = list(vec1)\n",
    "    vec2 = list(vec2)\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "\n",
    "    return dot_prod / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15967058203849993\n",
      "0.1832234081332565\n"
     ]
    }
   ],
   "source": [
    "print(cosine_sim(TD[:, 0], TD[:, 1])) #doc1 & doc2\n",
    "print(cosine_sim(TD[:, 0], TD[:, 2])) #doc1 & doc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penugasan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Buat vector space model dengan menggunakan sekumpulan dokumen pada folder ”berita”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from spacy.lang.id import Indonesian\n",
    "from spacy.lang.id.stop_words import STOP_WORDS\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "nlp = Indonesian()\n",
    "\n",
    "path = \"C:/Users/FEZA/My Drive/00. Drive PC/1.STIS/5. Semester 5/Information Retrieval [IR] P/Pertemuan 3/berita\"\n",
    "\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stemmer = StemmerFactory().create_stemmer()\n",
    "    stemmed_text = stemmer.stem(text)\n",
    "    \n",
    "    doc = nlp(stemmed_text)\n",
    "    tokens = [token.text for token in doc if token.text.lower() not in STOP_WORDS]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "inverted_index = {}\n",
    "doc_dict = {}\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if os.path.isfile(os.path.join(path, file)) and file.endswith(\".txt\"):\n",
    "        file_path = os.path.join(path, file)\n",
    "\n",
    "        text = read_text_file(file_path)\n",
    "\n",
    "        cleaned_tokens = preprocess_text(text)\n",
    "        doc_dict[file] = cleaned_tokens\n",
    "\n",
    "        # Ini utk inverted index nya\n",
    "        for token in set(cleaned_tokens):  # Menghindari duplikat\n",
    "            inverted_index.setdefault(token, []).append(file)\n",
    "        \n",
    "        vocab=list(inverted_index.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf_idf = tfidf(vocab, termFrequencyInDoc(vocab, doc_dict), inverseDocFre(vocab, wordDocFre(vocab, doc_dict), len(doc_dict)), doc_dict)\n",
    "\n",
    "# Term - Document Matrix\n",
    "TD = np.zeros((len(inverted_index), len(doc_dict)))\n",
    "for word in vocab:\n",
    "    for doc_id, doc in tf_idf.items():\n",
    "        ind1 = list(vocab).index(word)\n",
    "        ind2 = list(tf_idf.keys()).index(doc_id)\n",
    "        TD[ind1][ind2] = tf_idf[doc_id][word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 4.19722458  0.          0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 4.19722458  0.          0.          0.          0.        ]\n",
      " [ 1.40546511  1.40546511  1.40546511  0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 1.69314718  1.69314718  0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 1.          1.          1.          1.          1.        ]\n",
      " [ 1.69314718  1.69314718  0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 4.19722458  0.          0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 6.29583687  0.          0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 1.69314718  0.          0.          1.69314718  0.        ]\n",
      " [ 1.          1.          1.          1.          1.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 2.81093022  0.          2.81093022  1.40546511  0.        ]\n",
      " [ 3.          7.          4.          2.          3.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 6.29583687  0.          0.          0.          0.        ]\n",
      " [ 6.29583687  0.          0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 1.          1.          1.          1.          1.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 1.69314718  0.          0.          3.38629436  0.        ]\n",
      " [ 1.69314718  0.          3.38629436  0.          0.        ]\n",
      " [ 1.          1.          1.          1.          1.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 1.          1.          1.          1.          1.        ]\n",
      " [ 1.          1.          1.          4.          1.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 3.38629436  0.          0.          0.          1.69314718]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 1.          1.          1.          1.          1.        ]\n",
      " [ 3.38629436  0.          0.          0.          1.69314718]\n",
      " [ 1.69314718  0.          0.          1.69314718  0.        ]\n",
      " [ 4.19722458  0.          0.          0.          0.        ]\n",
      " [ 1.          1.          1.          1.          1.        ]\n",
      " [ 4.19722458  0.          0.          0.          0.        ]\n",
      " [16.         18.         19.         12.         16.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 4.19722458  0.          0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 1.69314718  0.          0.          0.          1.69314718]\n",
      " [ 3.          7.          4.          2.          3.        ]\n",
      " [ 4.19722458  0.          0.          0.          0.        ]\n",
      " [ 1.69314718  1.69314718  0.          0.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.          0.        ]\n",
      " [ 0.          1.69314718  5.07944154  0.          0.        ]\n",
      " [ 0.          1.69314718  1.69314718  0.          0.        ]\n",
      " [ 0.          6.29583687  0.          0.          0.        ]\n",
      " [ 0.          2.09861229  0.          0.          0.        ]\n",
      " [ 0.          1.69314718  1.69314718  0.          0.        ]\n",
      " [ 0.          1.69314718  1.69314718  0.          0.        ]\n",
      " [ 0.          4.19722458  0.          0.          0.        ]\n",
      " [ 0.          1.69314718  1.69314718  0.          0.        ]\n",
      " [ 0.          1.69314718  1.69314718  0.          0.        ]\n",
      " [ 0.          1.69314718  5.07944154  0.          0.        ]\n",
      " [ 0.          2.09861229  0.          0.          0.        ]\n",
      " [ 0.          2.09861229  0.          0.          0.        ]\n",
      " [ 0.          1.69314718  1.69314718  0.          0.        ]\n",
      " [ 0.          3.38629436  5.07944154  0.          0.        ]\n",
      " [ 0.          2.81093022  2.81093022  1.40546511  0.        ]\n",
      " [ 0.          2.09861229  0.          0.          0.        ]\n",
      " [ 0.          2.09861229  0.          0.          0.        ]\n",
      " [ 0.          1.69314718  1.69314718  0.          0.        ]\n",
      " [ 0.          2.09861229  0.          0.          0.        ]\n",
      " [ 0.          6.77258872  8.4657359   0.          0.        ]\n",
      " [ 0.          1.69314718  1.69314718  0.          0.        ]\n",
      " [ 0.          1.69314718  1.69314718  0.          0.        ]\n",
      " [ 0.          0.          2.09861229  0.          0.        ]\n",
      " [ 0.          0.          3.38629436  0.          1.69314718]\n",
      " [ 0.          0.          1.69314718  0.          3.38629436]\n",
      " [ 0.          0.          2.09861229  0.          0.        ]\n",
      " [ 0.          0.          2.09861229  0.          0.        ]\n",
      " [ 0.          0.          2.09861229  0.          0.        ]\n",
      " [ 0.          0.          2.09861229  0.          0.        ]\n",
      " [ 0.          0.          5.62186043  7.02732554  5.62186043]\n",
      " [ 0.          0.          7.02732554  9.83825576  2.81093022]\n",
      " [ 0.          0.          4.19722458  0.          0.        ]\n",
      " [ 0.          0.          2.09861229  0.          0.        ]\n",
      " [ 0.          0.          2.09861229  0.          0.        ]\n",
      " [ 0.          0.          4.19722458  0.          0.        ]\n",
      " [ 0.          0.          2.09861229  0.          0.        ]\n",
      " [ 0.          0.          4.19722458  0.          0.        ]\n",
      " [ 0.          0.          3.38629436  1.69314718  0.        ]\n",
      " [ 0.          0.          6.29583687  0.          0.        ]\n",
      " [ 0.          0.          6.29583687  0.          0.        ]\n",
      " [ 0.          0.          2.09861229  0.          0.        ]\n",
      " [ 0.          0.          2.09861229  0.          0.        ]\n",
      " [ 0.          0.          2.09861229  0.          0.        ]\n",
      " [ 0.          0.          1.69314718  1.69314718  0.        ]\n",
      " [ 0.          0.          2.09861229  0.          0.        ]\n",
      " [ 0.          0.          2.09861229  0.          0.        ]\n",
      " [ 0.          0.          0.          2.09861229  0.        ]\n",
      " [ 0.          0.          0.          2.09861229  0.        ]\n",
      " [ 0.          0.          0.          4.19722458  0.        ]\n",
      " [ 0.          0.          0.          2.09861229  0.        ]\n",
      " [ 0.          0.          0.          1.69314718  1.69314718]\n",
      " [ 0.          0.          0.          2.09861229  0.        ]\n",
      " [ 0.          0.          0.          2.09861229  0.        ]\n",
      " [ 0.          0.          0.         10.49306144  0.        ]\n",
      " [ 0.          0.          0.          2.09861229  0.        ]\n",
      " [ 0.          0.          0.          2.09861229  0.        ]\n",
      " [ 0.          0.          0.          2.09861229  0.        ]\n",
      " [ 0.          0.          0.          2.09861229  0.        ]\n",
      " [ 0.          0.          0.          2.09861229  0.        ]\n",
      " [ 0.          0.          0.          2.09861229  0.        ]\n",
      " [ 0.          0.          0.          2.09861229  0.        ]\n",
      " [ 0.          0.          0.          2.09861229  0.        ]\n",
      " [ 0.          0.          0.          2.09861229  0.        ]\n",
      " [ 0.          0.          0.          4.19722458  0.        ]\n",
      " [ 0.          0.          0.          2.09861229  0.        ]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          4.19722458]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          4.19722458]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          6.29583687]\n",
      " [ 0.          0.          0.          0.          4.19722458]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          4.19722458]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]\n",
      " [ 0.          0.          0.          0.          2.09861229]]\n"
     ]
    }
   ],
   "source": [
    "print(TD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Dari 5 file pada folder ”berita”, hitung skor kemiripan antara berita yang satu dan lainnya masing-masing dengan edit distance, jaccard similarity, euclidian distance, dan cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from spacy.lang.id import Indonesian\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from spacy.lang.id.stop_words import STOP_WORDS\n",
    "from scipy.spatial import distance\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Pada kode dibawah kita melakukan beberapa revisi pada function edit_distance, dimana nantinya akan terdapat seluruh operasi yang ada di edit distance seperti insert, substitusi, dan delete. Penggunaan library scipy digunakan untuk mempermudah penggunaan operasi perbedaan antar dokumen pada beberap fungsi dibawah sepertin distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stemmer = StemmerFactory().create_stemmer()\n",
    "    stemmed_text = stemmer.stem(text)\n",
    "    \n",
    "    doc = nlp(stemmed_text)\n",
    "    tokens = [token.text for token in doc if token.text.lower() not in STOP_WORDS]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def edit_distance(string1, string2):\n",
    "    # Implementasi edit distance\n",
    "    if len(string1) > len(string2):\n",
    "        string1, string2 = string1[:len(string2)], string2\n",
    "    elif len(string2) > len(string1):\n",
    "        string1, string2 = string1, string2[:len(string1)]\n",
    "\n",
    "    distance_matrix = [[0] * (len(string2) + 1) for _ in range(len(string1) + 1)]\n",
    "\n",
    "    for i in range(len(string1) + 1):\n",
    "        distance_matrix[i][0] = i\n",
    "\n",
    "    for j in range(len(string2) + 1):\n",
    "        distance_matrix[0][j] = j\n",
    "\n",
    "    for i in range(1, len(string1) + 1):\n",
    "        for j in range(1, len(string2) + 1):\n",
    "            cost = 0 if string1[i - 1] == string2[j - 1] else 1\n",
    "            distance_matrix[i][j] = min(\n",
    "                distance_matrix[i - 1][j] + 1,        # Delete\n",
    "                distance_matrix[i][j - 1] + 1,        # Insert\n",
    "                distance_matrix[i - 1][j - 1] + cost  # Substitusi\n",
    "            )\n",
    "\n",
    "    return distance_matrix[len(string1)][len(string2)]\n",
    "\n",
    "def jaccard_sim(list1, list2):\n",
    "    intersection = len(set(list1).intersection(list2))\n",
    "    union = len(set(list1).union(list2))\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "def euclidean_dist(vec1, vec2):\n",
    "    return distance.euclidean(vec1, vec2)\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    return 1 - distance.cosine(vec1, vec2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kemudian, kita lakukan double iterasi untuk membandingkan setiap dokumen satu dengan keseluruhan dokumen yang ada di folder berita. Total ada 10 kombinasi yang akan didapatkan dari iterasi berikut dan menghitung skor kesamaan dan perbedaan setiap dokumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kesamaan antara berita1.txt and berita2.txt:\n",
      "Edit Distance: 331\n",
      "Jaccard Similarity: 0.17857142857142858\n",
      "Euclidean Distance: 8.306623862918075\n",
      "Cosine Similarity: 0.31318038399724624\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kesamaan antara berita1.txt and berita3.txt:\n",
      "Edit Distance: 396\n",
      "Jaccard Similarity: 0.14\n",
      "Euclidean Distance: 9.273618495495704\n",
      "Cosine Similarity: 0.24656448378672147\n",
      "\n",
      "Kesamaan antara berita1.txt and berita4.txt:\n",
      "Edit Distance: 313\n",
      "Jaccard Similarity: 0.1744186046511628\n",
      "Euclidean Distance: 8.426149773176359\n",
      "Cosine Similarity: 0.3050444380432815\n",
      "\n",
      "Kesamaan antara berita1.txt and berita5.txt:\n",
      "Edit Distance: 338\n",
      "Jaccard Similarity: 0.13725490196078433\n",
      "Euclidean Distance: 9.38083151964686\n",
      "Cosine Similarity: 0.2419553954370992\n",
      "\n",
      "Kesamaan antara berita2.txt and berita3.txt:\n",
      "Edit Distance: 264\n",
      "Jaccard Similarity: 0.4126984126984127\n",
      "Euclidean Distance: 6.082762530298219\n",
      "Cosine Similarity: 0.5927489783638191\n",
      "\n",
      "Kesamaan antara berita2.txt and berita4.txt:\n",
      "Edit Distance: 260\n",
      "Jaccard Similarity: 0.1875\n",
      "Euclidean Distance: 7.211102550927978\n",
      "Cosine Similarity: 0.31589887589559384\n",
      "\n",
      "Kesamaan antara berita2.txt and berita5.txt:\n",
      "Edit Distance: 285\n",
      "Jaccard Similarity: 0.1375\n",
      "Euclidean Distance: 8.306623862918075\n",
      "Cosine Similarity: 0.24609055357847565\n",
      "\n",
      "Kesamaan antara berita3.txt and berita4.txt:\n",
      "Edit Distance: 312\n",
      "Jaccard Similarity: 0.22972972972972974\n",
      "Euclidean Distance: 7.54983443527075\n",
      "Cosine Similarity: 0.3774982529316784\n",
      "\n",
      "Kesamaan antara berita3.txt and berita5.txt:\n",
      "Edit Distance: 343\n",
      "Jaccard Similarity: 0.16483516483516483\n",
      "Euclidean Distance: 8.717797887081348\n",
      "Cosine Similarity: 0.28306925853614895\n",
      "\n",
      "Kesamaan antara berita4.txt and berita5.txt:\n",
      "Edit Distance: 284\n",
      "Jaccard Similarity: 0.17721518987341772\n",
      "Euclidean Distance: 8.06225774829855\n",
      "Cosine Similarity: 0.3050695435482862\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#perhitungan untuk setiap dokumen di berita\n",
    "for i, (file1, tokens1) in enumerate(doc_dict.items()):\n",
    "    for file2, tokens2 in list(doc_dict.items())[i+1:]:\n",
    "        edit_dist = edit_distance(' '.join(tokens1), ' '.join(tokens2))\n",
    "        jaccard_similarity = jaccard_sim(tokens1, tokens2)\n",
    "\n",
    "        # Ubah tokens menjadi vektor biner\n",
    "        vec1 = [1 if token in tokens1 else 0 for token in vocab]\n",
    "        vec2 = [1 if token in tokens2 else 0 for token in vocab]\n",
    "\n",
    "        euclidean_distance = euclidean_dist(vec1, vec2)\n",
    "        cosine_similarity = cosine_sim(vec1, vec2)\n",
    "\n",
    "        print(f\"Kesamaan antara {file1} and {file2}:\")\n",
    "        print(f\"Edit Distance: {edit_dist}\")\n",
    "        print(f\"Jaccard Similarity: {jaccard_similarity}\")\n",
    "        print(f\"Euclidean Distance: {euclidean_distance}\")\n",
    "        print(f\"Cosine Similarity: {cosine_similarity}\")\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
