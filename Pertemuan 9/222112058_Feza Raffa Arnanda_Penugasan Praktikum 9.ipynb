{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penugasan Praktikum Pertemuan 9 Information Retrieval\n",
    "\n",
    "Nama    : Feza Raffa Arnanda\n",
    "\n",
    "NIM     : 222112058\n",
    "\n",
    "Kelas   : 3SD2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soal\n",
    "Buat fungsi untuk menampilkan 3 list dokumen yang terurut pada folder ”berita” dengan query ”vaksin corona jakarta”, berdasarkan standar query likelihood model serta query likelihood model dengan Laplace Smoothing, Jelinek-Mercer Smoothing, dan Dirichlet Smoothing. Bandingkan dengan hasil perankingan BM25 pada modul 8 serta cosine similarity pada modul 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenisasi(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    # create stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    # stemming process\n",
    "    output = stemmer.stem(text)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_sentence(text):\n",
    "    stemmed_tokens = [stemming(token) for token in tokenisasi(text)]\n",
    "    output = ' '.join(stemmed_tokens)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "def stemming_sastrawi(tokens):\n",
    "    # Membuat stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_folding(text):\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('Indonesian'))\n",
    "\n",
    "def eliminasi_stopword(token):\n",
    "    return [kata for kata in token if kata not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path Folder Berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/FEZA/My Drive/00. Drive PC/1.STIS/5. Semester 5/Information Retrieval [IR] P/berita\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open file folder berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "inverted_index = {}\n",
    "doc_dict = {}\n",
    "i = 1\n",
    "for filename in os.listdir(path):\n",
    "    if (filename.endswith('.txt')):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        # Ekstrak angka dari nama file menggunakan regular expressions\n",
    "        match = re.search(r'\\d+', filename)\n",
    "        if match:\n",
    "            doc_id = match.group() # Mengambil angka dari nama file sebagai dokumen ID\n",
    "            with open (file_path, mode='r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                hasil_case_folding = case_folding(text)\n",
    "                token = tokenisasi(hasil_case_folding)\n",
    "                token_bersih = eliminasi_stopword(token)\n",
    "                stemm_token = stemming_sastrawi(token_bersih)\n",
    "                stemm_token_final = [item for item in stemm_token if item != ''] # membersihkan term kosong pada hasil stemming sebelumnya\n",
    "                # Menggabungkan hasil stemming menjadi sebuah teks/paragraf\n",
    "                doc_dict[doc_id] = ' '.join(stemm_token_final)\n",
    "                for term in set(stemm_token_final): # penggunaan set untuk mengantisipasi duplikasi term pada sebuah dokumen\n",
    "                    if term in inverted_index:\n",
    "                        inverted_index[term].append(doc_id)\n",
    "                    else:\n",
    "                        inverted_index[term] = [doc_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Query Likelihood Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact Top K Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def exact_top_k(doc_dict, rank_score, k):\n",
    "    relevance_scores = {}\n",
    "    # perubahan disini\n",
    "    rank_score = list(rank_score.values())\n",
    "    i = 0\n",
    "    for doc_id in doc_dict.keys():\n",
    "        relevance_scores[doc_id] = rank_score[i]\n",
    "        i = i + 1\n",
    "    sorted_value = OrderedDict(sorted(relevance_scores.items(), key=lambda x: x[1], reverse = True))\n",
    "    top_k = {j: sorted_value[j] for j in list(sorted_value)[:k]}\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"vaksin corona jakarta\"\n",
    "tokenized_query = tokenisasi(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0.0, '2': 0.0, '3': 1.1851851851851852e-05, '4': 0.0, '5': 0.0}\n"
     ]
    }
   ],
   "source": [
    "likelihood_scores = {}\n",
    "vocab = set()\n",
    "for doc_id in doc_dict.keys():\n",
    "    likelihood_scores[doc_id] = 1\n",
    "    tokens = tokenisasi(doc_dict[doc_id])\n",
    "    vocab.update(tokens)\n",
    "    for q in tokenized_query : \n",
    "        likelihood_scores[doc_id]=likelihood_scores[doc_id]*tokens.count(q)/len(tokens)\n",
    "print(likelihood_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: berita3.txt, Skor: 1.1851851851851852e-05\n",
      "Rank 2: berita1.txt, Skor: 0.0\n",
      "Rank 3: berita2.txt, Skor: 0.0\n",
      "Rank 4: berita4.txt, Skor: 0.0\n",
      "Rank 5: berita5.txt, Skor: 0.0\n"
     ]
    }
   ],
   "source": [
    "top_k_standard = exact_top_k(doc_dict,likelihood_scores,5)\n",
    "\n",
    "for i, (doc_id, score) in enumerate(top_k_standard.items()):\n",
    "    print(f\"Rank {i + 1}: berita{doc_id}.txt, Skor: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2', 'januari', 'dki', 'd-5816690', 'prof', 'catat', 'riset', 'balitbangkes', 'nasional', 'tahap', 'vaksin', 'amerika', 'corona-di-as-mendadak-naik-lagi-usai-serangan-delta-sempat-mereda', 'health', 'asal', 'dosis', 'gelombang', 'senin', 'cs', 'cek', 'rencana', 'idi', 'batas', 'corona', '2021', 'alert-kasus-varian-delta-covid-19-di-dki-meningkat', 'influenza', 'dadak', 'pasca', 'detik', '11', 'alpha', 'anthony', '34', 'tiga', 'pasien', 'satgas', 'd-5816582', 'd-5816534', '1-2', 'protokol', '165', 'hijau', 'berita-detikhealth', 'wilayah-kamu-sudah-bebas-covid-19-cek-34-kabkota-zona-hijau-terbaru', '327', '3', '-', 'sehat', 'total', 'perintah', 'rutin', 'gantung', 'd-5812940', 'p2pml', 'level', 'tarmizi', 'kepala', 'stabil', '86', 'ada', 'pakar', 'moderna', '13', 'varian', 'tingkat', 'pfizer', 'area', 'timur', 'as', 'terap', 'beri', '2022', 'https', 'picu', 'kait', 'alami', 'jelas', 'nasihat', 'ikut', 'kendali', 'masyarakat', 'menteri', 'desember', 'dr', 'tular', 'hitung', 'wilayah', 'alert', 'virus', 'bebas', 'ketua', 'musim', 'booster', 'jenis', 'tambah', 'puncak', '24', 'jawa', 'com', 'jakarta', 'indonesia', 'kota', 'd-5813949', 'panas', 'covid-19', 'turun', 'medis', 'giat', 'cegah', 'direktur', 'barat', 'suntik', 'djoerban', '15', 'baru', 'siti', 'minggu', 'kaji', 'utara', 'singgung', 'strain', 'lantas', 'zubairi', '1', 'ampuh', 'putih', 'laut', 'lawan', 'kab', 'nadia', 'aku', 'gedung', 'november', 'ikat', 'sakit', 'turut', 'fauci', 'bijak', 'longgar', 'laku', 'vaksin-covid-19-bakal-rutin-setiap-tahun-tergantung-ini-penjelasannya', 'data', 'serang', 'efektivitas', '57', 'pasti', 'sulawesi', 'bukti', '90', 'sebut', 'persen', 'signifikan', 'dokter', 'beta', 'delta', 'langsung', 'mobilitas', 'reda', 'kemenkes', 'serikat', 'vaksinasi', 'zona', 'dasar', 'ppkm', 'ri', 'ri-mulai-suntikkan-booster-di-2022-masihkah-ampuh-lawan-varian-delta-cs'}\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = [j for sub in [tokenisasi(doc_dict[doc_id]) for doc_id in doc_dict] for j in sub]\n",
    "vocab = set(tokenized_corpus)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 1.502397864551771e-07, '2': 8.049605695096029e-07, '3': 1.6934217901613323e-06, '4': 4.5087423160886264e-07, '5': 3.465250811388477e-07}\n"
     ]
    }
   ],
   "source": [
    "alpha = 1\n",
    "likelihood_scores = {}\n",
    "for doc_id in doc_dict.keys():\n",
    "    likelihood_scores[doc_id] = 1\n",
    "    tokens = tokenisasi(doc_dict[doc_id])\n",
    "    for q in tokenized_query:\n",
    "        likelihood_scores[doc_id]=likelihood_scores[doc_id]*(tokens.count(q)+alpha)/(len(tokens)+len(vocab)*alpha)\n",
    "print(likelihood_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: berita3.txt, Skor: 1.6934217901613323e-06\n",
      "Rank 2: berita2.txt, Skor: 8.049605695096029e-07\n",
      "Rank 3: berita4.txt, Skor: 4.5087423160886264e-07\n",
      "Rank 4: berita5.txt, Skor: 3.465250811388477e-07\n",
      "Rank 5: berita1.txt, Skor: 1.502397864551771e-07\n"
     ]
    }
   ],
   "source": [
    "top_k_laplace = exact_top_k(doc_dict,likelihood_scores,5)\n",
    "\n",
    "for i, (doc_id, score) in enumerate(top_k_laplace.items()):\n",
    "    print(f\"Rank {i + 1}: berita{doc_id}.txt, Skor: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jelinek-Mercer Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 8.487885870243773e-07, '2': 3.3613924700770085e-06, '3': 9.024832978282128e-06, '4': 2.0535207750589776e-06, '5': 3.266991684078316e-06}\n"
     ]
    }
   ],
   "source": [
    "lamda = 0.5\n",
    "likelihood_scores = {}\n",
    "for doc_id in doc_dict.keys():\n",
    "    likelihood_scores[doc_id] = 1\n",
    "    tokens = tokenisasi(doc_dict[doc_id])\n",
    "    for q in tokenized_query: \n",
    "        likelihood_scores[doc_id]=likelihood_scores[doc_id]*((lamda*tokens.count(q)/len(tokens))+((1-lamda)*tokenized_corpus.count(q)/len(tokenized_corpus)))\n",
    "print(likelihood_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: berita3.txt, Skor: 9.024832978282128e-06\n",
      "Rank 2: berita2.txt, Skor: 3.3613924700770085e-06\n",
      "Rank 3: berita5.txt, Skor: 3.266991684078316e-06\n",
      "Rank 4: berita4.txt, Skor: 2.0535207750589776e-06\n",
      "Rank 5: berita1.txt, Skor: 8.487885870243773e-07\n"
     ]
    }
   ],
   "source": [
    "top_k_jm = exact_top_k(doc_dict,likelihood_scores,5)\n",
    "\n",
    "for i, (doc_id, score) in enumerate(top_k_jm.items()):\n",
    "    print(f\"Rank {i + 1}: berita{doc_id}.txt, Skor: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dirichlet Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 1.9014081250546086e-09, '2': 3.3355748653437976e-07, '3': 1.1791929441312962e-05, '4': 1.4010654679151002e-08, '5': 2.438809804746205e-07}\n"
     ]
    }
   ],
   "source": [
    "miu = 2\n",
    "likelihood_scores = {}\n",
    "for doc_id in doc_dict.keys():\n",
    "    likelihood_scores[doc_id] = 1\n",
    "    tokens = tokenisasi(doc_dict[doc_id])\n",
    "    for q in tokenized_query:\n",
    "        likelihood_scores[doc_id]=likelihood_scores[doc_id]*(tokens.count(q)+miu*tokenized_corpus.count(q)/len(tokenized_corpus))/(len(tokens)+miu)\n",
    "print(likelihood_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: berita3.txt, Skor: 1.1791929441312962e-05\n",
      "Rank 2: berita2.txt, Skor: 3.3355748653437976e-07\n",
      "Rank 3: berita5.txt, Skor: 2.438809804746205e-07\n",
      "Rank 4: berita4.txt, Skor: 1.4010654679151002e-08\n",
      "Rank 5: berita1.txt, Skor: 1.9014081250546086e-09\n"
     ]
    }
   ],
   "source": [
    "top_k_dirichlet = exact_top_k(doc_dict,likelihood_scores,5)\n",
    "\n",
    "for i, (doc_id, score) in enumerate(top_k_dirichlet.items()):\n",
    "    print(f\"Rank {i + 1}: berita{doc_id}.txt, Skor: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kesimpulan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada penggunaan standard query likelihood, top 5 document adalah berita3, berita1, berita2, berita4, dan berita5. Standard query likelihood model akan mengalami probabilitas 0 ketika tidak ada query yang muncul dalam dokumen dan akan langsung menghasilkan skor 0 dan menandakan bahwa metode ini sangat sensitif dan tidak memberikan relevansi top K document secara benar.\n",
    "\n",
    "Ketika menggunakan Laplace smoothing, top 5 document adalah berita3, berita2, berita4, berita5, dan berita1.\n",
    "\n",
    "Sedangkan ketika menggunakan Jelinek-Mercer dan Dirichlet smoothing, top 5 documentnya adalah berita3, berita2, berita5, berita4, dan berita1.\n",
    "\n",
    "Perbedaan pada urutan ke 3 dan 4 pada Laplace dan Jelinek-Mercer,Dirichlet dengan perbedaan skor yang tidak terlalu jauh.\n",
    "\n",
    "Ketika kita bandingkan dengan BM25 dan Cosine Similarity pada top 3 document, BM25, Jelinek-Mercer, serta Dirichlet menghasilkan top 3 yang sama, yaitu berita3, berita2, dan berita5. Sedangkan standard likelihood dan Laplace menghasilkan yang berbeda. Penggunaan cosine similarity juga menghasilkan top 3 document yang berbeda, dimana top 3 document menggunakan cosine similarity adalah berita2, berita3, dan berita5. Masing-masing metode mempunyai asumsi dan penghitungan yang berbeda, pada penugasan ini kita membandingkan hasil top K document yang dihasilkan."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
