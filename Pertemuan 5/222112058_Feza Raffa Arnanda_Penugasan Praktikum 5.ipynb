{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nama : Feza Raffa Arnanda\n",
    "\n",
    "NIM : 222112058\n",
    "\n",
    "Kelas : 2KS5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Building Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenisasi(text):\n",
    "    tokens = text.split(\" \")\n",
    "    return tokens\n",
    "def stemming(text):\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    # create stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    # stemming process\n",
    "    output = stemmer.stem(text)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc1': 'kembang sistem informasi jadwal',\n",
       " 'doc2': 'kembang model analisis sentimen berita',\n",
       " 'doc3': 'analisis sistem input output',\n",
       " 'doc4': 'kembang sistem informasi akademik universitas',\n",
       " 'doc5': 'kembang sistem cari berita ekonomi',\n",
       " 'doc6': 'analisis sistem neraca nasional',\n",
       " 'doc7': 'kembang sistem informasi layan statistik',\n",
       " 'doc8': 'kembang sistem cari skripsi di universitas',\n",
       " 'doc9': 'analisis sentimen publik hadap perintah',\n",
       " 'doc10': 'kembang model klasifikasi sentimen berita'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stemming_sentence(text):\n",
    "    output = \"\"\n",
    "    for token in tokenisasi(text):\n",
    "        output = output + stemming(token) + \" \"\n",
    "    return output[:-1]\n",
    "doc_dict_raw = {}\n",
    "doc_dict_raw['doc1'] = \"pengembangan sistem informasi penjadwalan\"\n",
    "doc_dict_raw['doc2'] = \"pengembangan model analisis sentimen berita\"\n",
    "doc_dict_raw['doc3'] = \"analisis sistem input output\"\n",
    "doc_dict_raw['doc4'] = \"pengembangan sistem informasi akademik universitas\"\n",
    "doc_dict_raw['doc5'] = \"pengembangan sistem cari berita ekonomi\"\n",
    "doc_dict_raw['doc6'] = \"analisis sistem neraca nasional\"\n",
    "doc_dict_raw['doc7'] = \"pengembangan sistem informasi layanan statistik\"\n",
    "doc_dict_raw['doc8'] = \"pengembangan sistem pencarian skripsi di universitas\"\n",
    "doc_dict_raw['doc9'] = \"analisis sentimen publik terhadap pemerintah\"\n",
    "doc_dict_raw['doc10'] = \"pengembangan model klasifikasi sentimen berita\"\n",
    "doc_dict = {}\n",
    "for doc_id,doc in doc_dict_raw.items():\n",
    "    doc_dict[doc_id] = stemming_sentence(doc)\n",
    "doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kembang\n",
      "sistem\n",
      "informasi\n",
      "jadwal\n",
      "kembang\n",
      "model\n",
      "analisis\n",
      "sentimen\n",
      "berita\n",
      "analisis\n",
      "sistem\n",
      "input\n",
      "output\n",
      "kembang\n",
      "sistem\n",
      "informasi\n",
      "akademik\n",
      "universitas\n",
      "kembang\n",
      "sistem\n",
      "cari\n",
      "berita\n",
      "ekonomi\n",
      "analisis\n",
      "sistem\n",
      "neraca\n",
      "nasional\n",
      "kembang\n",
      "sistem\n",
      "informasi\n",
      "layan\n",
      "statistik\n",
      "kembang\n",
      "sistem\n",
      "cari\n",
      "skripsi\n",
      "di\n",
      "universitas\n",
      "analisis\n",
      "sentimen\n",
      "publik\n",
      "hadap\n",
      "perintah\n",
      "kembang\n",
      "model\n",
      "klasifikasi\n",
      "sentimen\n",
      "berita\n",
      "['kembang', 'sistem', 'informasi', 'jadwal', 'model', 'analisis', 'sentimen', 'berita', 'input', 'output', 'akademik', 'universitas', 'cari', 'ekonomi', 'neraca', 'nasional', 'layan', 'statistik', 'skripsi', 'di', 'publik', 'hadap', 'perintah', 'klasifikasi']\n",
      "{'kembang': ['doc1', 'doc2', 'doc4', 'doc5', 'doc7', 'doc8', 'doc10'], 'sistem': ['doc1', 'doc3', 'doc4', 'doc5', 'doc6', 'doc7', 'doc8'], 'informasi': ['doc1', 'doc4', 'doc7'], 'jadwal': ['doc1'], 'model': ['doc2', 'doc10'], 'analisis': ['doc2', 'doc3', 'doc6', 'doc9'], 'sentimen': ['doc2', 'doc9', 'doc10'], 'berita': ['doc2', 'doc5', 'doc10'], 'input': ['doc3'], 'output': ['doc3'], 'akademik': ['doc4'], 'universitas': ['doc4', 'doc8'], 'cari': ['doc5', 'doc8'], 'ekonomi': ['doc5'], 'neraca': ['doc6'], 'nasional': ['doc6'], 'layan': ['doc7'], 'statistik': ['doc7'], 'skripsi': ['doc8'], 'di': ['doc8'], 'publik': ['doc9'], 'hadap': ['doc9'], 'perintah': ['doc9'], 'klasifikasi': ['doc10']}\n"
     ]
    }
   ],
   "source": [
    "vocab = [] # Vocab ini isinya daftar kata yang sudah terstem\n",
    "inverted_index = {}\n",
    "for doc_id,doc in doc_dict.items():\n",
    "    for token in tokenisasi(doc):\n",
    "        print(token)\n",
    "        if token not in vocab:\n",
    "            vocab.append(token)\n",
    "            inverted_index[token] = []\n",
    "        if token in inverted_index:\n",
    "            if doc_id not in inverted_index[token]:\n",
    "                inverted_index[token].append(doc_id)\n",
    "print(vocab)\n",
    "print(inverted_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Exact Top K Document Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def wordDocFre(vocab, doc_dict):\n",
    "    df = {}\n",
    "    for word in vocab:\n",
    "        frq = 0\n",
    "        for doc in doc_dict.values():\n",
    "          #if word in doc.lower().split():\n",
    "            if word in doc:\n",
    "                frq = frq + 1\n",
    "        df[word] = frq\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termFrequencyInDoc(vocab, doc_dict):\n",
    "    tf_docs = {}\n",
    "    for doc_id in doc_dict.keys():\n",
    "        tf_docs[doc_id] = {}\n",
    "    for word in vocab:\n",
    "        for doc_id,doc in doc_dict.items():\n",
    "            tf_docs[doc_id][word] = doc.count(word)\n",
    "    return tf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverseDocFre(vocab,doc_fre,length):\n",
    "    idf= {}\n",
    "    for word in vocab:\n",
    "        idf[word] = idf[word] = 1 + np.log((length + 1) / (doc_fre[word]+1))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(vocab,tf,idf_scr,doc_dict):\n",
    "    tf_idf_scr = {}\n",
    "    for doc_id in doc_dict.keys():\n",
    "        tf_idf_scr[doc_id] = {}\n",
    "    for word in vocab:\n",
    "        for doc_id,doc in doc_dict.items():\n",
    "            tf_idf_scr[doc_id][word] = tf[doc_id][word] * idf_scr[word]\n",
    "    return tf_idf_scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.31845373 1.31845373 0.         1.31845373 1.31845373 0.\n",
      "  1.31845373 1.31845373 0.         1.31845373]\n",
      " [1.31845373 0.         1.31845373 1.31845373 1.31845373 1.31845373\n",
      "  1.31845373 1.31845373 0.         0.        ]\n",
      " [2.01160091 0.         0.         2.01160091 0.         0.\n",
      "  2.01160091 0.         0.         0.        ]\n",
      " [2.70474809 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         2.29928298 0.         0.         0.         0.\n",
      "  0.         0.         0.         2.29928298]\n",
      " [0.         1.78845736 1.78845736 0.         0.         1.78845736\n",
      "  0.         0.         1.78845736 0.        ]\n",
      " [0.         2.01160091 0.         0.         0.         0.\n",
      "  0.         0.         2.01160091 2.01160091]\n",
      " [0.         2.01160091 0.         0.         2.01160091 0.\n",
      "  0.         0.         0.         2.01160091]\n",
      " [0.         0.         2.70474809 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         2.70474809 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         2.70474809 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         2.29928298 0.         0.\n",
      "  0.         2.29928298 0.         0.        ]\n",
      " [0.         0.         0.         0.         2.29928298 0.\n",
      "  0.         2.29928298 0.         0.        ]\n",
      " [0.         0.         0.         0.         2.70474809 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         2.70474809\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         2.70474809\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  2.70474809 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  2.70474809 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         2.70474809 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         2.70474809 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         2.70474809 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         2.70474809 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         2.70474809 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         2.70474809]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf = tfidf(vocab, termFrequencyInDoc(vocab, doc_dict), inverseDocFre(vocab, wordDocFre(vocab, doc_dict), len(doc_dict)), doc_dict)\n",
    "# Term - Document Matrix\n",
    "TD = np.zeros((len(vocab), len(doc_dict)))\n",
    "for word in vocab:\n",
    "    for doc_id,doc in tf_idf.items():\n",
    "        ind1 = vocab.index(word)\n",
    "        ind2 = list(tf_idf.keys()).index(doc_id)\n",
    "        TD[ind1][ind2] = tf_idf[doc_id][word]\n",
    "print(TD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hitung terlebih dahulu bobot TF.IDF untuk suatu query sehingga didapatkan suatu term-query matrix. Misalkan terdapat suatu query ”sistem informasi statistik”, untuk mencari dokumen yang paling sesuai dari koleksi dokumen pada praktikum sebelumnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"sistem informasi statistik\"\n",
    "def termFrequency(vocab, query):\n",
    "    tf_query = {}\n",
    "    for word in vocab:\n",
    "        tf_query[word] = query.count(word)\n",
    "    return tf_query\n",
    "tf_query = termFrequency(vocab, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = inverseDocFre(vocab, wordDocFre(vocab, doc_dict),len(doc_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah itu, anda dapat menggunakan idf yang telah dihitung pada praktikum sebelumnya untuk mendapatkan bobot tf.idf dari query sehingga didapatkan suatu term-query matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [1.31845373]\n",
      " [2.01160091]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.70474809]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Term - Query Matrix\n",
    "TQ = np.zeros((len(vocab), 1)) # hanya 1 query\n",
    "for word in vocab:\n",
    "    ind1 = vocab.index(word)\n",
    "    TQ[ind1][0] = tf_query[word]*idf[word]\n",
    "print(TQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cosine_sim(vec1, vec2):\n",
    "    vec1 = list(vec1)\n",
    "    vec2 = list(vec2)\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "\n",
    "    return dot_prod / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.414904809442661\n",
      "0.0\n",
      "0.10856998991379904\n"
     ]
    }
   ],
   "source": [
    "print(cosine_sim(TQ[:, 0], TD[:, 0])) #query & doc1\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 1])) #query & doc2\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 2])) #query & doc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan hasil pengukuran kemiripan tersebut, urutkan dokumen dari skor tertinggi. Dokumen mana yang paling mirip dengan query tersebut? Kemudian lakukan analisis serupa untuk query berikut.\n",
    "1. sistem sentimen berita\n",
    "2. analisis jadwal universitas\n",
    "\n",
    "Anda dapat menyimpan skor kemiripan tersebut dalam suatu list dan mengambil k skor teratas untuk suatu query dengan kode berikut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exact Top K Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def exact_top_k(doc_dict, TD, q, k):\n",
    "    relevance_scores = {}\n",
    "    i = 0\n",
    "    for doc_id in doc_dict.keys():\n",
    "        relevance_scores[doc_id] = cosine_sim(q, TD[:, i])\n",
    "        i = i + 1\n",
    "    sorted_value = OrderedDict(sorted(relevance_scores.items(), key=lambda x: x[1], reverse = True))\n",
    "    top_k = {j: sorted_value[j] for j in list(sorted_value)[:k]}\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misalkan k=3, maka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc7': 0.7689768599816609, 'doc1': 0.414904809442661, 'doc4': 0.35626622628022314}\n"
     ]
    }
   ],
   "source": [
    "top_3 = exact_top_k(doc_dict, TD, TQ[:, 0], 3)\n",
    "print(top_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Inexact Top K Document Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk mendapatkan top k dokumen dengan index elimination, salah satu cara sederhananya adalah menghitung skor kemiripan pada dokumen yang minimal memiliki satu term yang cocok dengan query. Contohnya, untuk query ”sistem informasi statistik”, hanya dokumen 1, 3, 4, 5, 6, 7 dan dokumen 8 saja yang dihitung skornya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_elim_simple(query, doc_dict):\n",
    "    remove_list =[]\n",
    "    for doc_id,doc in doc_dict.items():\n",
    "        n = 0\n",
    "        for word in tokenisasi(query):\n",
    "            if stemming(word) in doc:\n",
    "                n = n+1\n",
    "        if n==0:\n",
    "            remove_list.append(doc_id)\n",
    "    for key in remove_list:\n",
    "        del doc_dict[key]\n",
    "    return doc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokumen yang akan dihitung skornya dapat dieliminasi dengan memanggil fungsi di atas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selain itu, term pada query yang digunakan untuk mengeliminasi dokumen juga dapat dikurangi dengan hanya menggunakan term dengan nilai idf yang besar, atau dengan batas nilai idf tertentu. Term pada query dapat dieliminasi dengan fungsi berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc1': 'kembang sistem informasi jadwal', 'doc3': 'analisis sistem input output', 'doc4': 'kembang sistem informasi akademik universitas', 'doc5': 'kembang sistem cari berita ekonomi', 'doc6': 'analisis sistem neraca nasional', 'doc7': 'kembang sistem informasi layan statistik', 'doc8': 'kembang sistem cari skripsi di universitas'}\n"
     ]
    }
   ],
   "source": [
    "query = \"sistem informasi statistik\"\n",
    "doc_dict = index_elim_simple(query, doc_dict)\n",
    "print(doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elim_query(query, idf_dict, idf_score):\n",
    "    for term in tokenisasi(query):\n",
    "        if idf_dict[stemming(term)]<idf_score:\n",
    "            query = query.replace(term+\" \", \"\")\n",
    "            query = query.replace(term, \"\")\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misalnya digunakan idf_score = 1.5 sebagai threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "informasi statistik\n"
     ]
    }
   ],
   "source": [
    "query = \"sistem informasi statistik\"\n",
    "query = elim_query(query, idf, 1.5)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Champion List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk setiap term pada vocabulary, hanya r dokumen dengan weight tertinggi saja yang dimasukkan ke dalam champion list. Hal ini berbeda dengan inverted index atau posting list yang berisi daftar seluruh dokumen dimana term tersebut berada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_championlist(inverted_index, tf_idf, r):\n",
    "    champion_list = {}\n",
    "    for term in inverted_index.keys():\n",
    "        weight_scores = {}\n",
    "        for doc_id,tf in tf_idf.items():\n",
    "            if tf_idf[doc_id][term]!=0:\n",
    "                weight_scores[doc_id] = tf_idf[doc_id][term]\n",
    "    sorted_value = OrderedDict(sorted(weight_scores.items(), key=lambda x: x[1],reverse = True))\n",
    "    top_r = {j: sorted_value[j] for j in list(sorted_value)[:r]}\n",
    "    champion_list[term]=list(top_r.keys())\n",
    "    return champion_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kemudian panggil fungsi di atas untuk mendapatkan champion list untuk r tertentu, misalnya r=2. Bandingkan isi champion list dan inverted index yang telah dibuat sebelumnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'klasifikasi': ['doc10']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=2\n",
    "create_championlist(inverted_index, tf_idf, r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penugasan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buat fungsi main untuk menampilkan 3 list dokumen yang terurut berdasarkan cosine similarity pada folder ”berita” dengan query ”vaksin corona jakarta”. dan optimalisasi menggunakan index elimination sederhana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from spacy.lang.id import Indonesian\n",
    "from spacy.lang.id.stop_words import STOP_WORDS\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "nlp = Indonesian()\n",
    "\n",
    "path = \"C:/Users/FEZA/My Drive/00. Drive PC/1.STIS/5. Semester 5/Information Retrieval [IR] P/Pertemuan 4/berita\"\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stemmer = StemmerFactory().create_stemmer()\n",
    "    stemmed_text = stemmer.stem(text)\n",
    "    \n",
    "    doc = nlp(stemmed_text)\n",
    "    tokens = [token.text for token in doc if token.text.lower() not in STOP_WORDS]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "import math\n",
    "def cosine_sim(vec1, vec2):\n",
    "    vec1 = list(vec1)\n",
    "    vec2 = list(vec2)\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "\n",
    "    return dot_prod / (mag_1 * mag_2)\n",
    "\n",
    "def index_elim_simple(query, doc_dict):\n",
    "    remove_list =[]\n",
    "    for doc_id,doc in doc_dict.items():\n",
    "        n = 0\n",
    "        for word in tokenisasi(query):\n",
    "            if stemming(word) in doc:\n",
    "                n = n+1\n",
    "        if n==0:\n",
    "            remove_list.append(doc_id)\n",
    "    for key in remove_list:\n",
    "        del doc_dict[key]\n",
    "    return doc_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode di atas merupakan function yang dibutuhkan untuk melakukan preprocessing data dan membangun inverted index serta dictionarynya. kemudian disertakan juga function untuk menghitung cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: berita3.txt, Cosine Similarity Score: 0.16942647665002367\n",
      "Rank 2: berita4.txt, Cosine Similarity Score: 0.1315903389919538\n",
      "Rank 3: berita2.txt, Cosine Similarity Score: 0.13067709337232916\n"
     ]
    }
   ],
   "source": [
    "def main(query, path):\n",
    "    inverted_index = {}\n",
    "    doc_dict = {}\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if os.path.isfile(os.path.join(path, file)) and file.endswith(\".txt\"):\n",
    "            file_path = os.path.join(path, file)\n",
    "\n",
    "            text = read_text_file(file_path)\n",
    "\n",
    "            cleaned_tokens = preprocess_text(text)\n",
    "            doc_dict[file] = cleaned_tokens\n",
    "\n",
    "            # Ini untuk inverted index nya\n",
    "            for token in set(cleaned_tokens):  # Menghindari duplikat\n",
    "                inverted_index.setdefault(token, []).append(file)\n",
    "\n",
    "    # Index Elimination\n",
    "    doc_dict = index_elim_simple(query, doc_dict)\n",
    "\n",
    "    # Menghitung cosine similarity untuk keseluruhan dokumen\n",
    "    query_vector = [1 if word in tokenisasi(query) else 0 for word in inverted_index.keys()]\n",
    "    cos_sim_scores = []\n",
    "\n",
    "    for doc_id, doc in doc_dict.items():\n",
    "        doc_vector = [doc.count(word) for word in inverted_index.keys()]\n",
    "        cos_sim = cosine_sim(query_vector, doc_vector)\n",
    "        cos_sim_scores.append((doc_id, cos_sim))\n",
    "\n",
    "    # Mengurutkan dokument berdasarkan score cosinus\n",
    "    sorted_docs = sorted(cos_sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Menampilkna top 3 dokumen teratas\n",
    "    for i, (doc_id, score) in enumerate(sorted_docs[:3]):\n",
    "        print(f\"Rank {i+1}: {doc_id}, Cosine Similarity Score: {score}\")\n",
    "\n",
    "# Panggil fungsi main dengan query \"vaksin corona jakarta\"\n",
    "query = \"vaksin corona jakarta\"\n",
    "main(query, path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada kode main di atas, mengambil dua parameter, yaitu query yang diinginkan dan path dari folder berita. langkah awal adalah pembangunan inverted index, kemudian dilakukan simple index eliminiation terlebih dahulu. setelah itu, masuk ke tahap penghitungan cosine similarity untuk keseluruhan dokumen dan didapatkan tiga top dokumen yang relevan dengan query berdasarkan cosine similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
